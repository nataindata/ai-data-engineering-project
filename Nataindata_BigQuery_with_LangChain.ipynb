{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataindata/ai-data-engineering-project/blob/main/Nataindata_BigQuery_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359697d5",
      "metadata": {
        "id": "359697d5"
      },
      "source": [
        "# AI Data Engineering Project for Beginners: LangChain ðŸ¦œï¸ðŸ”— + Vertex AI PaLM API on BigQuery\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> \"*AI is a big deal because it turns English into a programming language*\"\n"
      ],
      "metadata": {
        "id": "-1vGuY-nJSQm"
      },
      "id": "-1vGuY-nJSQm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hands-on tutorial will show you how you can add generative AI features to your own applications with just a few lines of code using LangChain and LLMs on Google Cloud. We will build together a sample Python application that will be able to understand and respond to human language queries about the relational data stored in your Data warehouse.\n",
        "\n",
        "* ðŸ† This could be a great feature if you want to enable/showcase to your management how to talk to the data in a natural way and make their life easier.\n",
        "\n",
        "* ðŸ¦¾ It's basically creating your own AI Data assistant\n",
        "---\n"
      ],
      "metadata": {
        "id": "JwwUIeAnEJDB"
      },
      "id": "JwwUIeAnEJDB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective\n",
        "\n",
        "After completing the steps in this notebook:\n",
        "- You will get a hands-on experience with using the open-source [LangChain framework](https://python.langchain.com/en/latest/index.html) to develop applications powered by large language models. LangChain makes it easier to develop and deploy applications against any LLM model in a vendor-agnostic manner.\n",
        "- You will learn about the powerful features in [Google PaLM models made available through Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview).\n",
        "\n",
        "\n",
        "## The goal:\n",
        "\n",
        "- Build a new AI-powered data chatbot, where users can describe their needs in simple English text, along with regular filters (like price, etc.)\n",
        "- To add new generative AI experiences in their data warehouse for business stakeholders.\n",
        "\n",
        "\n",
        "Dataset:\n",
        "This notebook uses an example of TheLook data - a fictitious eCommerce clothing site developed by the Looker team.\n",
        "\n",
        "- The dataset for this notebook has been sampled and created from a larger public retail dataset available at Bigquery `bigquery-public-data.thelook_ecommerce.inventory_items`."
      ],
      "metadata": {
        "id": "1huWcXiJGOlM"
      },
      "id": "1huWcXiJGOlM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you begin\n",
        "\n",
        ">âš ï¸ **Running this codelab will incur Google Cloud charges. You may also be billed for Vertex AI API usages.**\n",
        "\n",
        "Pre-requisities:\n",
        "- You need to have an active Google Cloud account to successfully complete this tutorial.\n",
        "- Make a copy of the notebook and save a copy in the Drive. The account is the same as your Google Cloud account\n",
        "- This sample notebook must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
        "- You can use an existing project. Alternatively, you can create a new Cloud project [with free trial cloud credits.](https://cloud.google.com/free/docs/gcp-free-tier)\n",
        "- At the end of the tutorial, you can optionally clean-up these resources to avoid further charges.\n"
      ],
      "metadata": {
        "id": "pGtUITKKrZNV"
      },
      "id": "pGtUITKKrZNV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using this interactive notebook\n",
        "\n",
        "Click the **run** icon on the top left corner â–¶ï¸  of each cell within this notebook.\n",
        "\n",
        "> ðŸ’¡ Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `âŒ˜ + Enter` on a Mac).\n",
        "\n",
        "> âš ï¸ **To avoid any errors**, wait for each cell to finish in their order before clicking the next â€œrunâ€ icon.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "23syIR8ermTq"
      },
      "id": "23syIR8ermTq"
    },
    {
      "cell_type": "markdown",
      "id": "641ad69c-52a6-4dd3-a9ad-a76540b45bb3",
      "metadata": {
        "id": "641ad69c-52a6-4dd3-a9ad-a76540b45bb3"
      },
      "source": [
        "# Install Vertex AI LLM SDK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ohPUPez8imvE",
      "metadata": {
        "id": "ohPUPez8imvE"
      },
      "outputs": [],
      "source": [
        "\n",
        "! pip install --user --quiet google-cloud-aiplatform==1.25.0\n",
        "! pip install --user --quiet google-api-python-client"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e48a6664-b606-4f8e-8f96-4568f36c13c4",
      "metadata": {
        "id": "e48a6664-b606-4f8e-8f96-4568f36c13c4"
      },
      "source": [
        "# Install LangChain & SQL Alchemy\n",
        "### LangChain is an open source framework that allows AI developers to combine LLM like PaLM with external sources of computation and data\n",
        "\n",
        "[Langchain explanation here](https://pitch.com/v/unleashing-the-power-of-langchain-in-data-warehouse-jytu7h)\n",
        "\n",
        "Large language models or LLMs such as ChatGPT/Vertex AI can answer questions about a lot of topics, but an LLM in isolation knows only what it was trained on, which doesn't include your personal/company data, such as if you're in a company and have proprietary documents not on the internet, as well as data or articles that were written after the LLM was trained.\n",
        "\n",
        "So wouldn't it be useful if you or others such as your colleagues can\n",
        "have a conversation with your own data and get questions answered using information from those documents and using an LLM?\n",
        "\n",
        "LangChain is an open-source developer framework for building LLM applications. LangChain consists of several modular components as well as more end-to-end templates. The modular components in LangChain include:\n",
        "- prompts,\n",
        "- models,\n",
        "- indexes,\n",
        "- chains,\n",
        "- and agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Vertex AI LLM SDK, langchain and dependencies\n",
        "! pip install --quiet google-cloud-aiplatform langchain==0.0.229 pydantic==1.10.8 typing-inspect==0.8.0 typing_extensions==4.5.0  transformers config --upgrade --user"
      ],
      "metadata": {
        "id": "oRXqjNnI6fcN"
      },
      "id": "oRXqjNnI6fcN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQLAlchemy is an open-source SQL toolkit and object-relational mapper for the Python programming language released under the MIT License."
      ],
      "metadata": {
        "id": "pH67hu_UwSyD"
      },
      "id": "pH67hu_UwSyD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d894a722-65fa-4b9c-8200-003d2009e166",
      "metadata": {
        "id": "d894a722-65fa-4b9c-8200-003d2009e166"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Below libraries are required to build a SQL engine for BigQuery\n",
        "!pip install --user SQLAlchemy==1.4.48 --quiet\n",
        "!pip install --user sqlalchemy-bigquery --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B-mPnZJdiwkg",
      "metadata": {
        "id": "B-mPnZJdiwkg"
      },
      "source": [
        "---\n",
        "\n",
        "#### âš ï¸ Do not forget to \"RESTART RUNTIME\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if you want to use this notebook go to file and then say \"Save a copy\" in Drive.\n",
        "Here we're going to authenticate ourselves"
      ],
      "metadata": {
        "id": "Vk8H_qkOs-LC"
      },
      "id": "Vk8H_qkOs-LC"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth as google_auth\n",
        "google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "yVG_3CNz7tK9"
      },
      "id": "yVG_3CNz7tK9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tq8CMcvBiywu",
      "metadata": {
        "id": "Tq8CMcvBiywu"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"sunny-emblem-404116\"  # @param {type:\"string\"} ## Update this with your project id\n",
        "LOCATION = \"us-central1\" # @param {type:\"string\"} ## Continue with us-central1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e406ffe8-c404-4464-b1fb-35e7beecf83c",
      "metadata": {
        "id": "e406ffe8-c404-4464-b1fb-35e7beecf83c"
      },
      "source": [
        "---\n",
        "\n",
        "#### âš ï¸ Do not forget to update the PROJECT_ID to your project\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6596d4d4-0501-4abf-aa56-683654c2985e",
      "metadata": {
        "id": "6596d4d4-0501-4abf-aa56-683654c2985e"
      },
      "source": [
        "# Vertex AI LLM wrapper for using with Langchain\n",
        "\n",
        "â—ï¸In GCP don't forget to enable \"Vertex AI API\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91fdaa3-bc11-427c-91df-771acdaaa535",
      "metadata": {
        "id": "a91fdaa3-bc11-427c-91df-771acdaaa535"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "print(f\"LangChain version: {langchain.__version__}\")\n",
        "\n",
        "# from a Vertex AI perspective you're getting the AI platform from Lang chain\n",
        "from google.cloud import aiplatform\n",
        "print(f\"Vertex AI SDK version: {aiplatform.__version__}\")\n",
        "\n",
        "# Initialize Vertex AI SDK\n",
        "import vertexai\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining LangChain Models using Vertex AI PaLM API for Text\n"
      ],
      "metadata": {
        "id": "cS68t2OMHrFJ"
      },
      "id": "cS68t2OMHrFJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CZllkRptjWDN",
      "metadata": {
        "id": "CZllkRptjWDN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from typing import Any, Mapping, List, Dict, Optional, Tuple, Union\n",
        "from pydantic import BaseModel, Extra, root_validator\n",
        "\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "\n",
        "\n",
        "# Here we create utils classes for model text-bison@001\n",
        "# We use private and public classes wrappers, to get the default parameters for calling Vertex AI API.\n",
        "\n",
        "# Inheriting from pydantic BaseModel allows you to define the types of the\n",
        "# class attributes (client, model_name, temperature, etc.).\n",
        "# Pydantic will automatically validate the data against these types, helping catch potential errors early in the development process.\n",
        "\n",
        "class _VertexCommon(BaseModel):\n",
        "    \"\"\"Wrapper around Vertex AI large language models.\n",
        "\n",
        "    To use, you should have the\n",
        "    ``google.cloud.aiplatform.private_preview.language_models`` python package\n",
        "    installed.\n",
        "    \"\"\"\n",
        "    client: Any = None #: :meta private:\n",
        "\n",
        "    # Defining default model_name \"text-bison@001\"\n",
        "    model_name: str = \"text-bison@001\"\n",
        "    \"\"\"Model name to use.\"\"\"\n",
        "\n",
        "    # temperature in the context of language models is a\n",
        "    # parameter that influences the randomness and creativity\n",
        "    temperature: float = 0.2\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    top_p: int = 0.8\n",
        "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
        "\n",
        "    top_k: int = 40\n",
        "    \"\"\"The number of highest probability tokens to keep for top-k filtering.\"\"\"\n",
        "\n",
        "    max_output_tokens: int = 200\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    @property\n",
        "    def _default_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the default parameters for calling Vertex AI API.\"\"\"\n",
        "        return {\n",
        "            \"temperature\": self.temperature,\n",
        "            \"top_p\": self.top_p,\n",
        "            \"top_k\": self.top_k,\n",
        "            \"max_output_tokens\": self.max_output_tokens\n",
        "        }\n",
        "\n",
        "    def _predict(self, prompt: str, stop: Optional[List[str]]) -> str:\n",
        "        res = self.client.predict(prompt, **self._default_params)\n",
        "        return self._enforce_stop_words(res.text, stop)\n",
        "\n",
        "    def _enforce_stop_words(self, text: str, stop: Optional[List[str]]) -> str:\n",
        "        if stop:\n",
        "            return enforce_stop_tokens(text, stop)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of llm.\"\"\"\n",
        "        return \"vertex_ai\"\n",
        "\n",
        "\n",
        "# VertexLLM class for LLM text model \"text-bison@001\", inherits from private _VertexCommon class\n",
        "# where model parameters are defined and from Langchain's LLM base class of Langchain\n",
        "class VertexLLM(_VertexCommon, LLM):\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the python package exists in environment.\"\"\"\n",
        "        try:\n",
        "            from vertexai.preview.language_models import TextGenerationModel\n",
        "        except ImportError:\n",
        "            raise ValueError(\n",
        "                \"Could not import Vertex AI LLM python package. \"\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            values[\"client\"] = TextGenerationModel.from_pretrained(values[\"model_name\"])\n",
        "        except AttributeError:\n",
        "            raise ValueError(\n",
        "                \"Could not set Vertex Text Model client.\"\n",
        "            )\n",
        "\n",
        "        return values\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"Call out to Vertex AI's create endpoint.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to pass into the model.\n",
        "\n",
        "        Returns:\n",
        "            The string generated by the model.\n",
        "        \"\"\"\n",
        "        return self._predict(prompt, stop)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create your input DataSet\n",
        "- Copying data from public BigQuery `bigquery-public-data.thelook_ecommerce.inventory_items` dataset into a personal project.\n",
        "\n",
        "TheLook is a fictitious eCommerce clothing site developed by the Looker team. The dataset contains information about customers, products, orders, logistics, web events and digital marketing campaigns. The contents of this dataset are synthetic"
      ],
      "metadata": {
        "id": "9Ur6nup7iKrq"
      },
      "id": "9Ur6nup7iKrq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112822f8-04b9-4d66-8272-ab86f08fca44",
      "metadata": {
        "id": "112822f8-04b9-4d66-8272-ab86f08fca44"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from google.cloud.bigquery import Client\n",
        "\n",
        "client = Client(project=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6882df-3185-4484-841b-d4fcf1bd3529",
      "metadata": {
        "id": "9e6882df-3185-4484-841b-d4fcf1bd3529"
      },
      "outputs": [],
      "source": [
        "# @title Specify Project details and location of the BQ table\n",
        "\n",
        "project_id = \"sunny-emblem-404116\"  # @param {type:\"string\"}\n",
        "location = \"us-central1\"  # @param {type:\"string\"}\n",
        "dataset_id = \"thelook_ecommerce\" # @param {type:\"string\"}\n",
        "table_name = \"inventory_items\" # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2260b9dd-6fc1-4fa0-9f67-79682ecf5546",
      "metadata": {
        "id": "2260b9dd-6fc1-4fa0-9f67-79682ecf5546"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "CREATE SCHEMA `{PROJECT_ID}.{dataset_id}`\n",
        "OPTIONS(\n",
        "  location=\"us\"\n",
        "  )\n",
        "\"\"\".format(\n",
        "    PROJECT_ID=PROJECT_ID, dataset_id=dataset_id\n",
        ")\n",
        "query_job = client.query(query)\n",
        "print(query_job.result())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe18f28-5c86-409a-820c-9453b5739eae",
      "metadata": {
        "id": "8fe18f28-5c86-409a-820c-9453b5739eae"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "create or replace table `{PROJECT_ID}.{dataset_id}.{table_name}`\n",
        "as\n",
        "select\n",
        "id,\n",
        "product_id,\n",
        "created_at,\n",
        "sold_at,\n",
        "cost,\n",
        "product_category,\n",
        "product_name,\n",
        "product_brand,\n",
        "product_retail_price,\n",
        "product_department,\n",
        "product_sku,\n",
        "product_distribution_center_id\n",
        "from `bigquery-public-data.thelook_ecommerce.inventory_items`\n",
        "\"\"\".format(\n",
        "    PROJECT_ID=PROJECT_ID, dataset_id=dataset_id, table_name=table_name\n",
        ")\n",
        "query_job = client.query(query)\n",
        "print(query_job.result())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL Alchemy\n",
        "SQL Alchemy provides you a method to create engines, write queries on top of any data set. In this case we are going to use BigQuery.\n",
        "\n",
        "The engine is needed for SQL Alchemy to be able to query the project ID and the data set ID & needed as `engine` in Langchain's SQLDatabase. Aka proxy between BigQuery and Langchain"
      ],
      "metadata": {
        "id": "hWBSHl7PjMhd"
      },
      "id": "hWBSHl7PjMhd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a70ec4f-abb2-4abb-b2cc-eb3ffa70476e",
      "metadata": {
        "id": "2a70ec4f-abb2-4abb-b2cc-eb3ffa70476e"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import *\n",
        "from sqlalchemy.engine import create_engine\n",
        "from sqlalchemy.schema import *\n",
        "import pandas as pd\n",
        "\n",
        "table_uri = f\"bigquery://{project_id}/{dataset_id}\"\n",
        "engine = create_engine(table_uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d0daf0-e982-4e0f-89f3-b72416ea74a2",
      "metadata": {
        "id": "b4d0daf0-e982-4e0f-89f3-b72416ea74a2"
      },
      "outputs": [],
      "source": [
        "query=f\"\"\"SELECT * FROM `{project_id}.{dataset_id}.{table_name}`\"\"\"\n",
        "engine.execute(query).first()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## SQLDatabaseChain\n",
        "In LangChain, SQLDatabaseChain refers to a built-in chain that allows you to interact with SQL databases. It essentially enables you to bridge the gap between natural language and structured data stored in SQL databases.\n",
        "\n",
        "Function:\n",
        "\n",
        "- Allows you to query databases using natural language, similar to asking questions in plain English.\n",
        "- Can be used to build chatbots, dashboards, and other applications that interact with SQL data.\n",
        "- Supports various SQL dialects through SQLAlchemy, including MySQL, PostgreSQL, and Oracle.\n",
        "\n",
        "To be able to query Tabular Data, we need to specify the connection, or `engine` and pass it through to an Langchain's SQLDatabase class\n",
        "And then we'll create a chain that take our LLM, and DB. I'm setting `verbose=True` and `return_intermediate_steps=True` so you can see what is happening underneath the hood.\n",
        "\n",
        "After, we will create a prompt template - it's one of the methods by which you could design a prompt and run it with parameters `top_k`, `table_info`, `input`"
      ],
      "metadata": {
        "id": "5auDxpL9jyGM"
      },
      "id": "5auDxpL9jyGM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a2edb0-b8e8-4314-8aff-63b47bce1672",
      "metadata": {
        "id": "05a2edb0-b8e8-4314-8aff-63b47bce1672"
      },
      "outputs": [],
      "source": [
        "from langchain import SQLDatabase, SQLDatabaseChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "llm = VertexLLM(verbose=True)\n",
        "\n",
        "def bq_qna(question):\n",
        "  #create SQLDatabase instance from BQ engine\n",
        "  db = SQLDatabase(engine=engine,metadata=MetaData(bind=engine),include_tables=[table_name])\n",
        "\n",
        "  #create SQL DB Chain with the initialized LLM and above SQLDB instance\n",
        "  db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True, return_intermediate_steps=True)\n",
        "\n",
        "  #Define prompt for BigQuery SQL\n",
        "  _googlesql_prompt = \"\"\"You are a BigQuery SQL expert. Given an input question, first create a syntactically correct BigQuery query to run, then look at the results of the query and return the answer to the input question.\n",
        "  Unless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per BigQuery SQL. You can order the results to return the most informative data in the database.\n",
        "  Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
        "  Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
        "  Use the following format:\n",
        "  Question: \"Question here\"\n",
        "  SQLQuery: \"SQL Query to run\"\n",
        "  SQLResult: \"Result of the SQLQuery\"\n",
        "  Answer: \"Final answer here\"\n",
        "  Only use the following tables:\n",
        "  {table_info}\n",
        "\n",
        "  If someone asks for specific month, use ActivityDate between current month's start date and current month's end date\n",
        "\n",
        "  Question: {input}\"\"\"\n",
        "\n",
        "  # passing parameters into PromptTemplate, input variables & template\n",
        "  BigQuerySQL_PROMPT = PromptTemplate(\n",
        "      input_variables=[\"input\", \"table_info\", \"top_k\"],\n",
        "      template=_googlesql_prompt,\n",
        "  )\n",
        "\n",
        "  # passing question to the prompt template\n",
        "  final_prompt = BigQuerySQL_PROMPT.format(input=question, table_info=table_name, top_k=10000)\n",
        "\n",
        "  # pass final prompt to SQL Chain\n",
        "  # again the DB chain is the LLM initialized from and on top of the\n",
        "  # database engine the SQL database instantiation and then DB chain to do a SQL Alchemy query\n",
        "  output = db_chain(final_prompt)\n",
        "\n",
        "\n",
        "  return output['result'], output['intermediate_steps'][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946de585-312b-43d2-9670-e3d6e8498951",
      "metadata": {
        "id": "946de585-312b-43d2-9670-e3d6e8498951"
      },
      "outputs": [],
      "source": [
        "#Testing 1\n",
        "bq_qna('Count total number of products which were sold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bc1a71-1aeb-424f-ae54-1f1c61a4a605",
      "metadata": {
        "id": "c1bc1a71-1aeb-424f-ae54-1f1c61a4a605"
      },
      "outputs": [],
      "source": [
        "#Testing 2\n",
        "bq_qna('what were the total sales for product with the name like Original Perry Suspenders in 2023')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f636b4-7919-480f-aa4e-962b70fe2b3e",
      "metadata": {
        "id": "89f636b4-7919-480f-aa4e-962b70fe2b3e"
      },
      "outputs": [],
      "source": [
        "#Testing 3\n",
        "bq_qna('Write an SQL query to extract the month from the date field, calculate the sum of monthly sales for product with the name like \"Micro Slim Fit Boxer\" in 2023, and display the results as product name, month name followed by sales for each month in 2023.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**References:**\n",
        "\n",
        "- Adapted from [LangChain Cookbook](https://github.com/gkamradt/langchain-tutorials) from [Greg Kamradt](https://twitter.com/GregKamradt)\n",
        "- [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)\n",
        "- [LangChain Python Documentation](https://python.langchain.com/en/latest/)\n"
      ],
      "metadata": {
        "id": "u3G3GRXl3xy2"
      },
      "id": "u3G3GRXl3xy2"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6596d4d4-0501-4abf-aa56-683654c2985e"
      ],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}